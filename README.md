# micrograd-from-scratch

## Description

This repository contains my personal implementation of micrograd, inspired by Andrej Karpathy's work. The goal of this project is to deepen my understanding of automatic differentiation and the fundamentals of neural networks by building a small autograd engine and a neural network library from scratch.

## Inspiration

This project is based on Andrej Karpathy's micrograd:
- Original repository: [https://github.com/karpathy/micrograd](https://github.com/karpathy/micrograd)
- YouTube explanation: [https://www.youtube.com/watch?v=VMj-3S1tku0](https://www.youtube.com/watch?v=VMj-3S1tku0)

## Project Goals

- Implement a basic autograd engine for scalar values
- Create simple neural network components (neurons, layers)
- Build and train a small multi-layer perceptron (MLP)
- Gain hands-on experience with backpropagation and gradient descent

## Structure

---- To fill ----

## Usage

---- To fill ----

## Learning Notes

---- To fill ----

## License

This project is open source and available under the [MIT License](LICENSE).

## Acknowledgements

Special thanks to Andrej Karpathy for the original micrograd implementation and the educational content that made this learning project possible.
